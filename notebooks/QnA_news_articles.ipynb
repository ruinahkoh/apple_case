{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1f7b9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994c95b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruinah/Documents/ISYE6740/project/api/myenv/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from langchain.chains import RetrievalQA, VectorDBQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain.vectorstores import DocArrayInMemorySearch, Chroma\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "924826ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data contains news articles related to technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ffaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tech_news_articles2.csv').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3ed11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'index':'id'})\n",
    "df[['text','id']].to_csv('../data/vectorstore.csv',index=False)\n",
    "# pd.read_csv('../data/vectorstore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7249846e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "file = '../data/vectorstore.csv'\n",
    "# file = 'data/tech_news_articles.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb8320f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../data/vectorstore.csv', 'row': 0}, page_content='text: The Sun’ll come out tomorrow, and you no longer have to bet your bottom dollar to be sure of it. Google’s DeepMind team released its latest weather prediction model this week, which outperforms a lea… [+6059 chars]\\nid: 0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c90a3ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruinah/Documents/ISYE6740/project/api/myenv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# define embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63b004a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is the most popular language?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5ab657",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e200726",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruinah/Documents/ISYE6740/project/api/myenv/lib/python3.9/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    embedding=embeddings,\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd0cc37",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/q_hpffn12_589xblfw6_4j1m0000gn/T/ipykernel_55005/316324751.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm_replacement_model = ChatOpenAI(temperature=0, model='gpt-4o-mini')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m llm_replacement_model \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mquery(query, llm_replacement_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "llm_replacement_model = ChatOpenAI(temperature=0, model='gpt-4o-mini')\n",
    "\n",
    "response = index.query(query, llm_replacement_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e6f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responds with the summary of the document most related to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae21f1ff",
   "metadata": {
    "height": 30,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa29dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"which article is most relevant to AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d347a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query, llm_replacement_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17cee2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The first blurp about two artificial intelligence pioneers being awarded the Nobel Prize for their work in machine learning is the most relevant to AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7187573",
   "metadata": {},
   "source": [
    "## Using Chroma Vectordb for RAG\n",
    "- We create a chorma vectordb and use parallel processing for faster processing when inserting documents into chroma\n",
    "- Then we create a Retrival Augmented Generation (RAG) system where it answers the query with the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "658be6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/q_hpffn12_589xblfw6_4j1m0000gn/T/ipykernel_34184/3157313866.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(embedding_function =embeddings,\n"
     ]
    }
   ],
   "source": [
    "#Place vectorDB under /tmp. It can be anywhere else\n",
    "# from langchain.vectorstores import Chroma\n",
    "persist_directory = \"/tmp/chromadb\"\n",
    "# vectordb = Chroma.from_documents(documents=list(documents[0:1]), embedding=embeddings,\n",
    "#                                  persist_directory=persist_directory)\n",
    "vectordb = Chroma(embedding_function =embeddings,\n",
    "                                 persist_directory=persist_directory)\n",
    "\n",
    "\n",
    "# vectordb.persist()\n",
    "# vectordb._collection.count()\n",
    "\n",
    "def batch_process(documents_arr, batch_size,):\n",
    "    for i in range(1, len(documents_arr), batch_size):\n",
    "        batch = documents_arr[i:i + batch_size]\n",
    "        add_to_chroma_database(batch)\n",
    "\n",
    "def add_to_chroma_database(batch):\n",
    "    vectordb.add_documents(documents=batch)\n",
    "    \n",
    "    \n",
    "batch_size = 50\n",
    "\n",
    "# batch_process(documents, batch_size, add_to_chroma_database)\n",
    "\n",
    "def form_batch(documents_arr, batch_size):\n",
    "    data_list = []\n",
    "    for i in range(1, len(documents_arr), batch_size):\n",
    "        data_list.append(documents_arr[i:i + batch_size])\n",
    "    return data_list\n",
    "\n",
    "data_list = form_batch(documents, 50)\n",
    "\n",
    "#this allows parallel processing and faster processing for inserting the articles into chroma\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(add_to_chroma_database, data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b0b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = form_batch(documents, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c006c0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "227da161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blurp: Deploying AI applications to the cloud is a crucial step in enhancing their accessibility, usability, and real-world impact. By transitioning AI apps from a local environment to the cloud, developers can ensure that their applications are easily accessible to…\n",
      "blurp: Hello everyone,\r\n",
      "I’m exploring ways to optimize [cloud storage][1] solutions using Wolfram Language and would love to hear your insights and experiences.\r\n",
      "I’ve been working with large datasets and am particularly interested in:\r\n",
      "1.Data Compression: Are there …\n",
      "blurp: The article highlights the critical need for robust cloud security amidst emerging threats like APTs, quantum computing risks, and ransomware-as-a-service. It details advancements like Zero Trust Architecture, AI and ML integration, Secure Access Service Edge…\n",
      "blurp: These cloud security statistics paint a worrying picture for businesses worldwide. Nearly one in two companies have reported security breaches, a statistic all the more disturbing considering that nearly half of...\n",
      "The post Why NordLayer is so Successful — Th…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query)\n",
    "[print(doc.page_content) for doc in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f9f0a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ca7a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Based only on the following context\n",
    "{context}\n",
    " - -\n",
    "Answer the question:{question} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f976a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Article Title | Summary |\n",
      "|--------------|---------|\n",
      "| AI Technology Advancements | This article discusses the latest advancements in AI technology, including new developments in machine learning, natural language processing, and computer vision. It explores how these advancements are shaping various industries and improving efficiency and productivity. |\n",
      "| Challenges of AI | This article delves into the challenges that AI technology faces, such as bias in algorithms, data privacy concerns, and ethical implications. It discusses how these challenges are being addressed by researchers and the industry to ensure responsible AI development. |\n",
      "| Opportunities in AI | This article highlights the opportunities that AI presents, such as improved healthcare diagnostics, autonomous vehicles, and personalized recommendations. It explores how businesses can leverage AI to gain a competitive edge and drive innovation in their respective fields. |\n"
     ]
    }
   ],
   "source": [
    "def query_rag(query, vectordb):\n",
    "    \"\"\"\n",
    "    Query a Retrieval-Augmented Generation (RAG) system using Chroma database and OpenAI.\n",
    "    Args:\n",
    "    - query_text (str): The text to query the RAG system with.\n",
    "    Returns:\n",
    "    - formatted_response (str): Formatted response including the generated text and sources.\n",
    "    - response_text (str): The generated response text.\n",
    "    \"\"\"\n",
    "    results = vectordb.similarity_search_with_relevance_scores(query, k=3)\n",
    "    \n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "\n",
    "    # Combine context from matching documents\n",
    "    context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    # Create prompt template using context and query text\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "\n",
    "    # Initialize OpenAI chat model\n",
    "    model = ChatOpenAI()\n",
    "\n",
    "    # Generate response text based on the prompt\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    # Get sources of the matching documents\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\n",
    "    # Format and return response including generated text and sources\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    \n",
    "    return response_text, formatted_response\n",
    "\n",
    "response_text, formatted_response = query_rag(prompt, vectordb)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534597e-4b0c-4563-a208-e2dd91064438",
   "metadata": {},
   "source": [
    "## Retreival QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8187aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    documents, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be1fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "# retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c94d22",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "llm_replacement_model = ChatOpenAI(temperature=0, model='gpt-4o-mini')\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm_replacement_model, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4769316",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "query =  \"Please list all your articles with the topic of AI in a table in markdown and summarize each one\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fc3c2f3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fba1a5db",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b58916",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590b337",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb587c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec249f1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64f166",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21322e7e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
